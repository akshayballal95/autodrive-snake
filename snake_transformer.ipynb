{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from torchvision.transforms import transforms, Compose, ToTensor, Resize, Normalize, CenterCrop\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnakeDataSet(Dataset):\n",
    "    def __init__(self, dataframe, root_dir, stack_size, transform = None):\n",
    "        self.stack_size = stack_size\n",
    "        self.key_frame = dataframe\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.key_frame) - self.stack_size *3\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.to_list()\n",
    "        try:\n",
    "            img_names = [os.path.join(self.root_dir, self.key_frame.iloc[idx + i, 0]) for i in range(self.stack_size)]\n",
    "            images = [Image.open(img_name) for img_name in img_names]\n",
    "            label = torch.tensor(self.key_frame.iloc[idx + self.stack_size, 1])\n",
    "            if self.transform:\n",
    "                images = [self.transform(image) for image in images]\n",
    "        except:\n",
    "            img_names = [os.path.join(self.root_dir, self.key_frame.iloc[0 + i, 0]) for i in range(self.stack_size)]\n",
    "            images = [Image.open(img_name) for img_name in img_names]\n",
    "            label = torch.tensor(self.key_frame.iloc[0 + self.stack_size, 1])\n",
    "            if self.transform:\n",
    "                images = [self.transform(image) for image in images]\n",
    "        return torch.stack(images), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Compose([\n",
    "    Resize((64,64), antialias=True),\n",
    "    CenterCrop(64),\n",
    "    ToTensor(),\n",
    "    Normalize(mean =[0.485, 0.456, 0.406], std =[0.229, 0.224, 0.225] )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "STACK_SIZE = 4\n",
    "\n",
    "\n",
    "train, test = train_test_split(pd.read_csv(\"data/labels_snake.csv\"), test_size=0.1, shuffle=False)\n",
    "classes = [\"n\", \"left\", \"up\", \"right\", \"down\"]\n",
    "\n",
    "labels_unique, counts = np.unique(train[\"class\"], return_counts=True)\n",
    "class_weights = [sum(counts)/c for c in counts]\n",
    "example_weights = np.array([class_weights[l] for l in train['class']])\n",
    "example_weights = np.roll(example_weights, -STACK_SIZE)\n",
    "sampler = WeightedRandomSampler(example_weights, len(train))\n",
    "\n",
    "labels_unique, counts = np.unique(test[\"class\"], return_counts=True)\n",
    "class_weights = [sum(counts)/c for c in counts]\n",
    "test_example_weights = np.array([class_weights[l] for l in test['class']])\n",
    "test_example_weights = np.roll(test_example_weights, -STACK_SIZE)\n",
    "test_sampler = WeightedRandomSampler(test_example_weights, len(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE = 128\n",
    "dataset = SnakeDataSet(root_dir=\"captures\", dataframe = train, stack_size=STACK_SIZE, transform=transformer)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=sampler, drop_last= True)\n",
    "test_dataset = SnakeDataSet(root_dir=\"captures\", dataframe = test, stack_size=STACK_SIZE,  transform=transformer)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler = test_sampler, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 1, 4, 4, 2, 4, 1, 1, 2, 3, 0, 3, 1, 0, 0, 2, 3, 0, 4, 2, 2, 1, 3, 1,\n",
      "        2, 0, 4, 1, 4, 2, 3, 2, 4, 2, 0, 0, 4, 0, 1, 3, 4, 2, 1, 0, 4, 4, 0, 4,\n",
      "        4, 0, 3, 3, 4, 2, 0, 3, 0, 0, 2, 0, 2, 3, 4, 0, 4, 1, 4, 4, 1, 4, 2, 1,\n",
      "        4, 4, 0, 1, 0, 0, 4, 3, 3, 4, 0, 2, 1, 0, 3, 0, 4, 0, 4, 3, 4, 1, 4, 4,\n",
      "        1, 2, 3, 2, 1, 2, 0, 0, 0, 4, 3, 2, 0, 1, 4, 3, 2, 3, 0, 2, 2, 3, 4, 1,\n",
      "        1, 0, 0, 1, 0, 1, 4, 4])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, label = next(iter(dataloader))\n",
    "print(label)\n",
    "from matplotlib.pyplot import subplots\n",
    "fig,ax = subplots(1, STACK_SIZE, constrained_layout = True, figsize=(15,5))\n",
    "for i in range(STACK_SIZE):\n",
    "    ax[i].imshow(images[2][i].permute(1,2,0));\n",
    "    ax[i].axis('off')\n",
    "fig.supylabel(classes[label[2].item()]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The authors of the original transformer paper describe very succinctly what\n",
    "    the positional encoding layer does and why it is needed:\n",
    "\n",
    "    \"Since our model contains no recurrence and no convolution, in order for the\n",
    "    model to make use of the order of the sequence, we must inject some\n",
    "    information about the relative or absolute position of the tokens in the\n",
    "    sequence.\" (Vaswani et al, 2017)\n",
    "    Adapted from:\n",
    "    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dropout: float=0.1,\n",
    "        max_seq_len: int=5000,\n",
    "        d_model: int=512,\n",
    "        batch_first: bool=False\n",
    "        ):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            dropout: the dropout rate\n",
    "            max_seq_len: the maximum length of the input sequences\n",
    "            d_model: The dimension of the output of sub-layers in the model\n",
    "                     (Vaswani et al, 2017)\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        # adapted from PyTorch tutorial\n",
    "        position = torch.arange(max_seq_len).unsqueeze(1)\n",
    "\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        if self.batch_first:\n",
    "            pe = torch.zeros(1, max_seq_len, d_model)\n",
    "\n",
    "            pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "\n",
    "            pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        else:\n",
    "            pe = torch.zeros(max_seq_len, 1, d_model)\n",
    "\n",
    "            pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "\n",
    "            pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, enc_seq_len, dim_val] or\n",
    "               [enc_seq_len, batch_size, dim_val]\n",
    "        \"\"\"\n",
    "        if self.batch_first:\n",
    "            x = x + self.pe[:,:x.size(1)]\n",
    "        else:\n",
    "            x = x + self.pe[:x.size(0)]\n",
    "\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnakeConvolver(nn.Module):\n",
    "    def __init__(self, in_channels, stack_size, kernel_size, stride, hidden_channels = 128):\n",
    "        super(SnakeConvolver,self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.stack_size = stack_size\n",
    "        self.hidden_channels = hidden_channels\n",
    "\n",
    "        self.convblock = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, self.hidden_channels, kernel_size, stride),\n",
    "            nn.MaxPool2d(kernel_size, kernel_size),\n",
    "            nn.BatchNorm2d(self.hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(self.hidden_channels, self.hidden_channels*2, kernel_size, stride),\n",
    "            nn.MaxPool2d(kernel_size, kernel_size),\n",
    "            nn.BatchNorm2d(self.hidden_channels*2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.positionencoder = PositionalEncoder(d_model = self.hidden_channels*2, max_seq_len=self.stack_size+1, batch_first=True)\n",
    "        self.class_token_embeddings = nn.Parameter(torch.rand((BATCH_SIZE, 1, hidden_channels*2), requires_grad=True))\n",
    "\n",
    "    def forward(self,x):\n",
    "        embeds = torch.stack([self.convblock(x[:,i,:,:,:]) for i in range(self.stack_size)],dim=1).squeeze()\n",
    "        out = torch.cat((self.class_token_embeddings,embeds),dim=1)\n",
    "        out = self.positionencoder(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnakeMSA(nn.Module):\n",
    "    def __init__(self, embed_dims, num_heads, dropout = 0.5):\n",
    "        super(SnakeMSA, self).__init__()\n",
    "        self.embed_dims = embed_dims\n",
    "        self.num_heads = num_heads\n",
    "        self.layernorm = nn.LayerNorm(normalized_shape = self.embed_dims)\n",
    "        self.attentionblock = nn.MultiheadAttention(embed_dims, num_heads, dropout=dropout, batch_first=True)\n",
    "    def forward(self, x):\n",
    "        x = self.layernorm(x)\n",
    "        x = self.attentionblock(x, x, x, need_weights = False)[0]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, embed_dims, hidden_units, dropout = 0.3):\n",
    "        super(MLP, self).__init__()\n",
    "        self.embed_dims = embed_dims\n",
    "        self.hidden_units = hidden_units\n",
    "        self.layernorm = nn.LayerNorm(embed_dims)\n",
    "        self.mlp = nn.Sequential(\n",
    "                                        nn.Linear(self.embed_dims,self.hidden_units), \n",
    "                                        nn.GELU(),\n",
    "                                        nn.Dropout(dropout),\n",
    "                                        nn.Linear(self.hidden_units, self.embed_dims),\n",
    "                                        nn.Dropout(dropout),                                      \n",
    "                                        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(self.layernorm(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnakeTransformer(nn.Module):\n",
    "    def __init__(self, embed_dims, num_heads, hidden_units=256, msa_dropout=0.5, mlp_dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embed_dims = embed_dims\n",
    "        self.num_heads = num_heads\n",
    "        self.msa_dropout = msa_dropout\n",
    "        self.mlp_dropout = mlp_dropout\n",
    "        self.hidden_units = hidden_units\n",
    "        self.msa = SnakeMSA(embed_dims, num_heads, msa_dropout)\n",
    "        self.mlp = MLP(embed_dims, hidden_units, mlp_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.msa(x) + x\n",
    "        x = self.mlp(x) + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnakeModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels=3, \n",
    "                 stack_size = STACK_SIZE,\n",
    "                 kernel_size =3, \n",
    "                 stride=2, \n",
    "                 embed_dims = 768, \n",
    "                 num_transformer_blocks = 12,  \n",
    "                 num_heads = 12, \n",
    "                 msa_dropout = 0.0, \n",
    "                 mlp_dropout = 0.1, \n",
    "                 hidden_channels = 384,\n",
    "                 hidden_units=3072, \n",
    "                 output_size=5):\n",
    "        super(SnakeModel, self).__init__()\n",
    "        self.stack_size = stack_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.embed_dims = embed_dims\n",
    "        self.num_heads = num_heads\n",
    "        self.msa_dropout = msa_dropout\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.hidden_units = hidden_units\n",
    "        self.output_size = output_size\n",
    "        self.in_channels = in_channels\n",
    "        self.mlp_dropout = mlp_dropout\n",
    "        self.num_transformer_blocks = num_transformer_blocks\n",
    "        self.encoder = SnakeConvolver(in_channels, stack_size, kernel_size, stride, hidden_channels=  self.hidden_channels)\n",
    "        self.transformer = nn.Sequential(*[SnakeTransformer(self.embed_dims, \n",
    "                                                            self.num_heads, \n",
    "                                                            self.hidden_units,\n",
    "                                                            self.msa_dropout,\n",
    "                                                            self.mlp_dropout,\n",
    "                                                            ) for _ in range(self.num_transformer_blocks)])\n",
    "        self.classifier = nn.Sequential(nn.LayerNorm(embed_dims),\n",
    "                                        nn.Linear(embed_dims, output_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.classifier(self.transformer(x)[:,0])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 4, 3, 64, 64])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "SnakeModel                                    [128, 5]                  --\n",
       "├─SnakeConvolver: 1-1                         [128, 5, 768]             98,304\n",
       "│    └─Sequential: 2-1                        [128, 768, 1, 1]          --\n",
       "│    │    └─Conv2d: 3-1                       [128, 384, 31, 31]        10,752\n",
       "│    │    └─MaxPool2d: 3-2                    [128, 384, 10, 10]        --\n",
       "│    │    └─BatchNorm2d: 3-3                  [128, 384, 10, 10]        768\n",
       "│    │    └─ReLU: 3-4                         [128, 384, 10, 10]        --\n",
       "│    │    └─Conv2d: 3-5                       [128, 768, 4, 4]          2,654,976\n",
       "│    │    └─MaxPool2d: 3-6                    [128, 768, 1, 1]          --\n",
       "│    │    └─BatchNorm2d: 3-7                  [128, 768, 1, 1]          1,536\n",
       "│    │    └─ReLU: 3-8                         [128, 768, 1, 1]          --\n",
       "│    └─Sequential: 2-2                        [128, 768, 1, 1]          (recursive)\n",
       "│    │    └─Conv2d: 3-9                       [128, 384, 31, 31]        (recursive)\n",
       "│    │    └─MaxPool2d: 3-10                   [128, 384, 10, 10]        --\n",
       "│    │    └─BatchNorm2d: 3-11                 [128, 384, 10, 10]        (recursive)\n",
       "│    │    └─ReLU: 3-12                        [128, 384, 10, 10]        --\n",
       "│    │    └─Conv2d: 3-13                      [128, 768, 4, 4]          (recursive)\n",
       "│    │    └─MaxPool2d: 3-14                   [128, 768, 1, 1]          --\n",
       "│    │    └─BatchNorm2d: 3-15                 [128, 768, 1, 1]          (recursive)\n",
       "│    │    └─ReLU: 3-16                        [128, 768, 1, 1]          --\n",
       "│    └─Sequential: 2-3                        [128, 768, 1, 1]          (recursive)\n",
       "│    │    └─Conv2d: 3-17                      [128, 384, 31, 31]        (recursive)\n",
       "│    │    └─MaxPool2d: 3-18                   [128, 384, 10, 10]        --\n",
       "│    │    └─BatchNorm2d: 3-19                 [128, 384, 10, 10]        (recursive)\n",
       "│    │    └─ReLU: 3-20                        [128, 384, 10, 10]        --\n",
       "│    │    └─Conv2d: 3-21                      [128, 768, 4, 4]          (recursive)\n",
       "│    │    └─MaxPool2d: 3-22                   [128, 768, 1, 1]          --\n",
       "│    │    └─BatchNorm2d: 3-23                 [128, 768, 1, 1]          (recursive)\n",
       "│    │    └─ReLU: 3-24                        [128, 768, 1, 1]          --\n",
       "│    └─Sequential: 2-4                        [128, 768, 1, 1]          (recursive)\n",
       "│    │    └─Conv2d: 3-25                      [128, 384, 31, 31]        (recursive)\n",
       "│    │    └─MaxPool2d: 3-26                   [128, 384, 10, 10]        --\n",
       "│    │    └─BatchNorm2d: 3-27                 [128, 384, 10, 10]        (recursive)\n",
       "│    │    └─ReLU: 3-28                        [128, 384, 10, 10]        --\n",
       "│    │    └─Conv2d: 3-29                      [128, 768, 4, 4]          (recursive)\n",
       "│    │    └─MaxPool2d: 3-30                   [128, 768, 1, 1]          --\n",
       "│    │    └─BatchNorm2d: 3-31                 [128, 768, 1, 1]          (recursive)\n",
       "│    │    └─ReLU: 3-32                        [128, 768, 1, 1]          --\n",
       "│    └─PositionalEncoder: 2-5                 [128, 5, 768]             --\n",
       "│    │    └─Dropout: 3-33                     [128, 5, 768]             --\n",
       "├─Sequential: 1-2                             [128, 5, 768]             --\n",
       "│    └─SnakeTransformer: 2-6                  [128, 5, 768]             --\n",
       "│    │    └─SnakeMSA: 3-34                    [128, 5, 768]             --\n",
       "│    │    │    └─LayerNorm: 4-1               [128, 5, 768]             1,536\n",
       "│    │    │    └─MultiheadAttention: 4-2      [128, 5, 768]             2,362,368\n",
       "│    │    └─MLP: 3-35                         [128, 5, 768]             --\n",
       "│    │    │    └─LayerNorm: 4-3               [128, 5, 768]             1,536\n",
       "│    │    │    └─Sequential: 4-4              [128, 5, 768]             --\n",
       "│    │    │    │    └─Linear: 5-1             [128, 5, 3072]            2,362,368\n",
       "│    │    │    │    └─GELU: 5-2               [128, 5, 3072]            --\n",
       "│    │    │    │    └─Dropout: 5-3            [128, 5, 3072]            --\n",
       "│    │    │    │    └─Linear: 5-4             [128, 5, 768]             2,360,064\n",
       "│    │    │    │    └─Dropout: 5-5            [128, 5, 768]             --\n",
       "│    └─SnakeTransformer: 2-7                  [128, 5, 768]             --\n",
       "│    │    └─SnakeMSA: 3-36                    [128, 5, 768]             --\n",
       "│    │    │    └─LayerNorm: 4-5               [128, 5, 768]             1,536\n",
       "│    │    │    └─MultiheadAttention: 4-6      [128, 5, 768]             2,362,368\n",
       "│    │    └─MLP: 3-37                         [128, 5, 768]             --\n",
       "│    │    │    └─LayerNorm: 4-7               [128, 5, 768]             1,536\n",
       "│    │    │    └─Sequential: 4-8              [128, 5, 768]             --\n",
       "│    │    │    │    └─Linear: 5-6             [128, 5, 3072]            2,362,368\n",
       "│    │    │    │    └─GELU: 5-7               [128, 5, 3072]            --\n",
       "│    │    │    │    └─Dropout: 5-8            [128, 5, 3072]            --\n",
       "│    │    │    │    └─Linear: 5-9             [128, 5, 768]             2,360,064\n",
       "│    │    │    │    └─Dropout: 5-10           [128, 5, 768]             --\n",
       "│    └─SnakeTransformer: 2-8                  [128, 5, 768]             --\n",
       "│    │    └─SnakeMSA: 3-38                    [128, 5, 768]             --\n",
       "│    │    │    └─LayerNorm: 4-9               [128, 5, 768]             1,536\n",
       "│    │    │    └─MultiheadAttention: 4-10     [128, 5, 768]             2,362,368\n",
       "│    │    └─MLP: 3-39                         [128, 5, 768]             --\n",
       "│    │    │    └─LayerNorm: 4-11              [128, 5, 768]             1,536\n",
       "│    │    │    └─Sequential: 4-12             [128, 5, 768]             --\n",
       "│    │    │    │    └─Linear: 5-11            [128, 5, 3072]            2,362,368\n",
       "│    │    │    │    └─GELU: 5-12              [128, 5, 3072]            --\n",
       "│    │    │    │    └─Dropout: 5-13           [128, 5, 3072]            --\n",
       "│    │    │    │    └─Linear: 5-14            [128, 5, 768]             2,360,064\n",
       "│    │    │    │    └─Dropout: 5-15           [128, 5, 768]             --\n",
       "│    └─SnakeTransformer: 2-9                  [128, 5, 768]             --\n",
       "│    │    └─SnakeMSA: 3-40                    [128, 5, 768]             --\n",
       "│    │    │    └─LayerNorm: 4-13              [128, 5, 768]             1,536\n",
       "│    │    │    └─MultiheadAttention: 4-14     [128, 5, 768]             2,362,368\n",
       "│    │    └─MLP: 3-41                         [128, 5, 768]             --\n",
       "│    │    │    └─LayerNorm: 4-15              [128, 5, 768]             1,536\n",
       "│    │    │    └─Sequential: 4-16             [128, 5, 768]             --\n",
       "│    │    │    │    └─Linear: 5-16            [128, 5, 3072]            2,362,368\n",
       "│    │    │    │    └─GELU: 5-17              [128, 5, 3072]            --\n",
       "│    │    │    │    └─Dropout: 5-18           [128, 5, 3072]            --\n",
       "│    │    │    │    └─Linear: 5-19            [128, 5, 768]             2,360,064\n",
       "│    │    │    │    └─Dropout: 5-20           [128, 5, 768]             --\n",
       "│    └─SnakeTransformer: 2-10                 [128, 5, 768]             --\n",
       "│    │    └─SnakeMSA: 3-42                    [128, 5, 768]             --\n",
       "│    │    │    └─LayerNorm: 4-17              [128, 5, 768]             1,536\n",
       "│    │    │    └─MultiheadAttention: 4-18     [128, 5, 768]             2,362,368\n",
       "│    │    └─MLP: 3-43                         [128, 5, 768]             --\n",
       "│    │    │    └─LayerNorm: 4-19              [128, 5, 768]             1,536\n",
       "│    │    │    └─Sequential: 4-20             [128, 5, 768]             --\n",
       "│    │    │    │    └─Linear: 5-21            [128, 5, 3072]            2,362,368\n",
       "│    │    │    │    └─GELU: 5-22              [128, 5, 3072]            --\n",
       "│    │    │    │    └─Dropout: 5-23           [128, 5, 3072]            --\n",
       "│    │    │    │    └─Linear: 5-24            [128, 5, 768]             2,360,064\n",
       "│    │    │    │    └─Dropout: 5-25           [128, 5, 768]             --\n",
       "│    └─SnakeTransformer: 2-11                 [128, 5, 768]             --\n",
       "│    │    └─SnakeMSA: 3-44                    [128, 5, 768]             --\n",
       "│    │    │    └─LayerNorm: 4-21              [128, 5, 768]             1,536\n",
       "│    │    │    └─MultiheadAttention: 4-22     [128, 5, 768]             2,362,368\n",
       "│    │    └─MLP: 3-45                         [128, 5, 768]             --\n",
       "│    │    │    └─LayerNorm: 4-23              [128, 5, 768]             1,536\n",
       "│    │    │    └─Sequential: 4-24             [128, 5, 768]             --\n",
       "│    │    │    │    └─Linear: 5-26            [128, 5, 3072]            2,362,368\n",
       "│    │    │    │    └─GELU: 5-27              [128, 5, 3072]            --\n",
       "│    │    │    │    └─Dropout: 5-28           [128, 5, 3072]            --\n",
       "│    │    │    │    └─Linear: 5-29            [128, 5, 768]             2,360,064\n",
       "│    │    │    │    └─Dropout: 5-30           [128, 5, 768]             --\n",
       "│    └─SnakeTransformer: 2-12                 [128, 5, 768]             --\n",
       "│    │    └─SnakeMSA: 3-46                    [128, 5, 768]             --\n",
       "│    │    │    └─LayerNorm: 4-25              [128, 5, 768]             1,536\n",
       "│    │    │    └─MultiheadAttention: 4-26     [128, 5, 768]             2,362,368\n",
       "│    │    └─MLP: 3-47                         [128, 5, 768]             --\n",
       "│    │    │    └─LayerNorm: 4-27              [128, 5, 768]             1,536\n",
       "│    │    │    └─Sequential: 4-28             [128, 5, 768]             --\n",
       "│    │    │    │    └─Linear: 5-31            [128, 5, 3072]            2,362,368\n",
       "│    │    │    │    └─GELU: 5-32              [128, 5, 3072]            --\n",
       "│    │    │    │    └─Dropout: 5-33           [128, 5, 3072]            --\n",
       "│    │    │    │    └─Linear: 5-34            [128, 5, 768]             2,360,064\n",
       "│    │    │    │    └─Dropout: 5-35           [128, 5, 768]             --\n",
       "│    └─SnakeTransformer: 2-13                 [128, 5, 768]             --\n",
       "│    │    └─SnakeMSA: 3-48                    [128, 5, 768]             --\n",
       "│    │    │    └─LayerNorm: 4-29              [128, 5, 768]             1,536\n",
       "│    │    │    └─MultiheadAttention: 4-30     [128, 5, 768]             2,362,368\n",
       "│    │    └─MLP: 3-49                         [128, 5, 768]             --\n",
       "│    │    │    └─LayerNorm: 4-31              [128, 5, 768]             1,536\n",
       "│    │    │    └─Sequential: 4-32             [128, 5, 768]             --\n",
       "│    │    │    │    └─Linear: 5-36            [128, 5, 3072]            2,362,368\n",
       "│    │    │    │    └─GELU: 5-37              [128, 5, 3072]            --\n",
       "│    │    │    │    └─Dropout: 5-38           [128, 5, 3072]            --\n",
       "│    │    │    │    └─Linear: 5-39            [128, 5, 768]             2,360,064\n",
       "│    │    │    │    └─Dropout: 5-40           [128, 5, 768]             --\n",
       "│    └─SnakeTransformer: 2-14                 [128, 5, 768]             --\n",
       "│    │    └─SnakeMSA: 3-50                    [128, 5, 768]             --\n",
       "│    │    │    └─LayerNorm: 4-33              [128, 5, 768]             1,536\n",
       "│    │    │    └─MultiheadAttention: 4-34     [128, 5, 768]             2,362,368\n",
       "│    │    └─MLP: 3-51                         [128, 5, 768]             --\n",
       "│    │    │    └─LayerNorm: 4-35              [128, 5, 768]             1,536\n",
       "│    │    │    └─Sequential: 4-36             [128, 5, 768]             --\n",
       "│    │    │    │    └─Linear: 5-41            [128, 5, 3072]            2,362,368\n",
       "│    │    │    │    └─GELU: 5-42              [128, 5, 3072]            --\n",
       "│    │    │    │    └─Dropout: 5-43           [128, 5, 3072]            --\n",
       "│    │    │    │    └─Linear: 5-44            [128, 5, 768]             2,360,064\n",
       "│    │    │    │    └─Dropout: 5-45           [128, 5, 768]             --\n",
       "│    └─SnakeTransformer: 2-15                 [128, 5, 768]             --\n",
       "│    │    └─SnakeMSA: 3-52                    [128, 5, 768]             --\n",
       "│    │    │    └─LayerNorm: 4-37              [128, 5, 768]             1,536\n",
       "│    │    │    └─MultiheadAttention: 4-38     [128, 5, 768]             2,362,368\n",
       "│    │    └─MLP: 3-53                         [128, 5, 768]             --\n",
       "│    │    │    └─LayerNorm: 4-39              [128, 5, 768]             1,536\n",
       "│    │    │    └─Sequential: 4-40             [128, 5, 768]             --\n",
       "│    │    │    │    └─Linear: 5-46            [128, 5, 3072]            2,362,368\n",
       "│    │    │    │    └─GELU: 5-47              [128, 5, 3072]            --\n",
       "│    │    │    │    └─Dropout: 5-48           [128, 5, 3072]            --\n",
       "│    │    │    │    └─Linear: 5-49            [128, 5, 768]             2,360,064\n",
       "│    │    │    │    └─Dropout: 5-50           [128, 5, 768]             --\n",
       "│    └─SnakeTransformer: 2-16                 [128, 5, 768]             --\n",
       "│    │    └─SnakeMSA: 3-54                    [128, 5, 768]             --\n",
       "│    │    │    └─LayerNorm: 4-41              [128, 5, 768]             1,536\n",
       "│    │    │    └─MultiheadAttention: 4-42     [128, 5, 768]             2,362,368\n",
       "│    │    └─MLP: 3-55                         [128, 5, 768]             --\n",
       "│    │    │    └─LayerNorm: 4-43              [128, 5, 768]             1,536\n",
       "│    │    │    └─Sequential: 4-44             [128, 5, 768]             --\n",
       "│    │    │    │    └─Linear: 5-51            [128, 5, 3072]            2,362,368\n",
       "│    │    │    │    └─GELU: 5-52              [128, 5, 3072]            --\n",
       "│    │    │    │    └─Dropout: 5-53           [128, 5, 3072]            --\n",
       "│    │    │    │    └─Linear: 5-54            [128, 5, 768]             2,360,064\n",
       "│    │    │    │    └─Dropout: 5-55           [128, 5, 768]             --\n",
       "│    └─SnakeTransformer: 2-17                 [128, 5, 768]             --\n",
       "│    │    └─SnakeMSA: 3-56                    [128, 5, 768]             --\n",
       "│    │    │    └─LayerNorm: 4-45              [128, 5, 768]             1,536\n",
       "│    │    │    └─MultiheadAttention: 4-46     [128, 5, 768]             2,362,368\n",
       "│    │    └─MLP: 3-57                         [128, 5, 768]             --\n",
       "│    │    │    └─LayerNorm: 4-47              [128, 5, 768]             1,536\n",
       "│    │    │    └─Sequential: 4-48             [128, 5, 768]             --\n",
       "│    │    │    │    └─Linear: 5-56            [128, 5, 3072]            2,362,368\n",
       "│    │    │    │    └─GELU: 5-57              [128, 5, 3072]            --\n",
       "│    │    │    │    └─Dropout: 5-58           [128, 5, 3072]            --\n",
       "│    │    │    │    └─Linear: 5-59            [128, 5, 768]             2,360,064\n",
       "│    │    │    │    └─Dropout: 5-60           [128, 5, 768]             --\n",
       "├─Sequential: 1-3                             [128, 5]                  --\n",
       "│    └─LayerNorm: 2-18                        [128, 768]                1,536\n",
       "│    └─Linear: 2-19                           [128, 5]                  3,845\n",
       "===============================================================================================\n",
       "Total params: 87,826,181\n",
       "Trainable params: 87,826,181\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 34.30\n",
       "===============================================================================================\n",
       "Input size (MB): 25.17\n",
       "Forward/backward pass size (MB): 2053.38\n",
       "Params size (MB): 237.52\n",
       "Estimated Total Size (MB): 2316.06\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SnakeModel()\n",
    "summary(model, (BATCH_SIZE,STACK_SIZE,3, 64,64), depth=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LinearLR\n",
    "num_epochs = 7\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), 10e-6, weight_decay=0.1,)\n",
    "scheduler = LinearLR(optimizer, start_factor=1, end_factor=0.1, total_iters=num_epochs)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 5])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(images.to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/7: 100%|██████████| 1098/1098 [34:39<00:00,  1.89s/it, Loss=0.00761, Accuracy=0.605]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7, Val Loss: 0.0117, Val Accuracy: 0.5039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/7: 100%|██████████| 1098/1098 [33:29<00:00,  1.83s/it, Loss=0.0055, Accuracy=0.718] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/7, Val Loss: 0.0132, Val Accuracy: 0.4963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/7:  45%|████▌     | 496/1098 [15:22<18:39,  1.86s/it, Loss=0.0049, Accuracy=0.75]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [48], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Update statistics\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(torch\u001b[38;5;241m.\u001b[39msoftmax(outputs,\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     30\u001b[0m correct_predictions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (predicted \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "steps = 0\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    val_loss = 0.0\n",
    "    val_correct_predictions = 0\n",
    "    val_total_samples = 0\n",
    "\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # tqdm bar for progress visualization\n",
    "    pbar = tqdm(dataloader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=True)\n",
    "    for inputs, labels in pbar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs.to(device))\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update statistics\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(torch.softmax(outputs,1), 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        # Update tqdm bar with current loss and accuracy\n",
    "        pbar.set_postfix({'Loss': total_loss / total_samples, 'Accuracy': correct_predictions / total_samples})\n",
    "        steps = steps + 1\n",
    "        # if steps>1000:\n",
    "        #     for param_group in optimizer.param_groups:\n",
    "        #         param_group['lr'] = 2e-5\n",
    "\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs.to(device))\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Update statistics\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(torch.softmax(outputs,1), 1)\n",
    "            val_correct_predictions += (predicted == labels).sum().item()\n",
    "            val_total_samples += labels.size(0)\n",
    "\n",
    "    # Calculate and print epoch-level accuracy and loss for validation\n",
    "    epoch_loss = val_loss / val_total_samples\n",
    "    epoch_accuracy = val_correct_predictions / val_total_samples\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Val Loss: {epoch_loss:.4f}, Val Accuracy: {epoch_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"weights.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:49<00:00,  2.54it/s, Loss=0.175, Accuracy=0.35] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Loss: 0.1752, Accuracy: 0.3505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "total_loss = 0.0\n",
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.inference_mode():\n",
    "  # Set model to training mode\n",
    "  model.eval()\n",
    "\n",
    "  # tqdm bar for progress visualization\n",
    "  pbar = tqdm(test_dataloader, leave=True)\n",
    "\n",
    "  with torch.inference_mode(): \n",
    "    \n",
    "    for inputs, labels in pbar:\n",
    "        # Forward pass\n",
    "        inputs, labels = inputs.to(device), labels.type(torch.LongTensor).to(device)\n",
    "        outputs = model(inputs.to(device))\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Update statistics\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        # Update tqdm bar with current loss and accuracy\n",
    "        pbar.set_postfix({'Loss': total_loss / total_samples, 'Accuracy': correct_predictions / total_samples})\n",
    "\n",
    "    # Calculate and print epoch-level accuracy and loss\n",
    "    epoch_loss = total_loss / total_samples\n",
    "    epoch_accuracy = correct_predictions / total_samples\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.8871,  5.5179,  0.0093, -6.6027, -0.3690],\n",
       "        [ 0.4526, -5.7828, -0.3614,  4.9467, -0.7812],\n",
       "        [ 3.1903,  5.2011,  0.3370, -7.4490, -1.9668],\n",
       "        [ 0.3016, -2.6541,  2.7760,  2.0058, -3.3939],\n",
       "        [ 1.0856, -7.8013, -0.5289,  6.2668, -1.5776],\n",
       "        [ 3.0884, -2.2665, -6.6093, -0.0931,  4.0961],\n",
       "        [ 0.4661, -7.0749, -2.2711,  6.2445,  0.9774],\n",
       "        [ 3.7324,  1.8034, -5.9827, -4.3991,  3.6110],\n",
       "        [ 1.7292, -6.6276, -3.5998,  4.8401,  1.3216],\n",
       "        [ 3.2603,  1.6302, -6.6735, -3.7937,  4.7435],\n",
       "        [ 4.2501,  1.9277, -4.3518, -5.0623,  1.4290],\n",
       "        [ 2.0982,  4.6273,  2.6636, -6.1395, -3.5514],\n",
       "        [ 1.4905, -0.5644,  4.0618, -0.9971, -5.4521],\n",
       "        [ 0.6972,  1.3567, -4.2940, -1.5220,  4.5105],\n",
       "        [ 1.4132,  1.1621, -4.2883, -1.9620,  3.8141],\n",
       "        [ 1.9964, -7.1878, -0.9924,  4.9488, -1.7965]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images, label = next(iter(dataloader))\n",
    "model(images.to(device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autodrive-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

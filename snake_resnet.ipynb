{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from torchvision.transforms import transforms, Compose, ToTensor, Resize, Normalize, CenterCrop, Grayscale\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import math\n",
    "from torchvision.models.video import r3d_18, R3D_18_Weights, mc3_18, MC3_18_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnakeDataSet(Dataset):\n",
    "    def __init__(self, dataframe, root_dir, stack_size, transform = None):\n",
    "        self.stack_size = stack_size\n",
    "        self.key_frame = dataframe\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.key_frame) - self.stack_size *3\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.to_list()\n",
    "        try:\n",
    "            img_names = [os.path.join(self.root_dir, self.key_frame.iloc[idx + i, 0]) for i in range(self.stack_size)]\n",
    "            images = [Image.open(img_name) for img_name in img_names]\n",
    "            label = torch.tensor(self.key_frame.iloc[idx + self.stack_size, 1])\n",
    "            if self.transform:\n",
    "                images = [self.transform(image) for image in images]\n",
    "        except:\n",
    "            img_names = [os.path.join(self.root_dir, self.key_frame.iloc[0 + i, 0]) for i in range(self.stack_size)]\n",
    "            images = [Image.open(img_name) for img_name in img_names]\n",
    "            label = torch.tensor(self.key_frame.iloc[0 + self.stack_size, 1])\n",
    "            if self.transform:\n",
    "                images = [self.transform(image) for image in images]\n",
    "        return torch.stack(images,dim = 1).squeeze(), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Compose([\n",
    "    Resize((84,84), antialias=True),\n",
    "    CenterCrop(84),\n",
    "    ToTensor(),\n",
    "    Normalize(mean =[ 0.8725,  1.8742, -0.2931], std =[0.3376, 0.3561, 0.3825] )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "STACK_SIZE = 4\n",
    "\n",
    "train, test = train_test_split(pd.read_csv(\"data/labels_snake.csv\"), test_size=0.2, shuffle=False)\n",
    "classes = [\"n\", \"left\", \"up\", \"right\", \"down\"]\n",
    "\n",
    "labels_unique, counts = np.unique(train[\"class\"], return_counts=True)\n",
    "class_weights = [sum(counts)/c for c in counts]\n",
    "example_weights = np.array([class_weights[l] for l in train['class']])\n",
    "example_weights = np.roll(example_weights, -STACK_SIZE)\n",
    "sampler = WeightedRandomSampler(example_weights, len(train))\n",
    "\n",
    "labels_unique, counts = np.unique(test[\"class\"], return_counts=True)\n",
    "class_weights = [sum(counts)/c for c in counts]\n",
    "test_example_weights = np.array([class_weights[l] for l in test['class']])\n",
    "test_example_weights = np.roll(test_example_weights, -STACK_SIZE)\n",
    "test_sampler = WeightedRandomSampler(test_example_weights, len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE = 32\n",
    "dataset = SnakeDataSet(root_dir=\"captures\", dataframe = train, stack_size=STACK_SIZE, transform=transformer)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=sampler, drop_last= True)\n",
    "test_dataset = SnakeDataSet(root_dir=\"captures\", dataframe = test, stack_size=STACK_SIZE,  transform=transformer)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler = test_sampler, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_std(dataloader):\n",
    "    '''\n",
    "    We assume that the images of the dataloader have the same height and width\n",
    "    source: https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/Basics/pytorch_std_mean.py\n",
    "    '''\n",
    "    # var[X] = E[X**2] - E[X]**2\n",
    "    channels_sum, channels_sqrd_sum, num_batches = 0, 0, 0\n",
    "\n",
    "    for batch_images, labels in tqdm(dataloader):  # (B,H,W,C)\n",
    "        batch_images = batch_images.permute(0,3,4,2,1)\n",
    "        channels_sum += torch.mean(batch_images, dim=[0, 1, 2, 3])\n",
    "        channels_sqrd_sum += torch.mean(batch_images ** 2, dim=[0, 1, 2,3])\n",
    "        num_batches += 1\n",
    "\n",
    "    mean = channels_sum / num_batches\n",
    "    std = (channels_sqrd_sum / num_batches - mean ** 2) ** 0.5\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_mean_std(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, label = next(iter(dataloader))\n",
    "# print(label)\n",
    "# from matplotlib.pyplot import subplots\n",
    "# fig,ax = subplots(1, STACK_SIZE, constrained_layout = True, figsize=(15,5))\n",
    "# for i in range(STACK_SIZE):\n",
    "#     ax[i].imshow(images[2][i,:,:],cmap=\"gray\");\n",
    "#     ax[i].axis('off')\n",
    "# fig.supylabel(classes[label[2].item()]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 4, 84, 84])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "VideoResNet                              [32, 5]                   --\n",
       "├─BasicStem: 1-1                         [32, 64, 4, 42, 42]       --\n",
       "│    └─Conv3d: 2-1                       [32, 64, 4, 42, 42]       28,224\n",
       "│    └─BatchNorm3d: 2-2                  [32, 64, 4, 42, 42]       128\n",
       "│    └─ReLU: 2-3                         [32, 64, 4, 42, 42]       --\n",
       "├─Sequential: 1-2                        [32, 64, 4, 42, 42]       --\n",
       "│    └─BasicBlock: 2-4                   [32, 64, 4, 42, 42]       --\n",
       "│    │    └─Sequential: 3-1              [32, 64, 4, 42, 42]       110,720\n",
       "│    │    └─Sequential: 3-2              [32, 64, 4, 42, 42]       110,720\n",
       "│    │    └─ReLU: 3-3                    [32, 64, 4, 42, 42]       --\n",
       "│    └─BasicBlock: 2-5                   [32, 64, 4, 42, 42]       --\n",
       "│    │    └─Sequential: 3-4              [32, 64, 4, 42, 42]       110,720\n",
       "│    │    └─Sequential: 3-5              [32, 64, 4, 42, 42]       110,720\n",
       "│    │    └─ReLU: 3-6                    [32, 64, 4, 42, 42]       --\n",
       "├─Sequential: 1-3                        [32, 128, 2, 21, 21]      --\n",
       "│    └─BasicBlock: 2-6                   [32, 128, 2, 21, 21]      --\n",
       "│    │    └─Sequential: 3-7              [32, 128, 2, 21, 21]      221,440\n",
       "│    │    └─Sequential: 3-8              [32, 128, 2, 21, 21]      442,624\n",
       "│    │    └─Sequential: 3-9              [32, 128, 2, 21, 21]      8,448\n",
       "│    │    └─ReLU: 3-10                   [32, 128, 2, 21, 21]      --\n",
       "│    └─BasicBlock: 2-7                   [32, 128, 2, 21, 21]      --\n",
       "│    │    └─Sequential: 3-11             [32, 128, 2, 21, 21]      442,624\n",
       "│    │    └─Sequential: 3-12             [32, 128, 2, 21, 21]      442,624\n",
       "│    │    └─ReLU: 3-13                   [32, 128, 2, 21, 21]      --\n",
       "├─Sequential: 1-4                        [32, 256, 1, 11, 11]      --\n",
       "│    └─BasicBlock: 2-8                   [32, 256, 1, 11, 11]      --\n",
       "│    │    └─Sequential: 3-14             [32, 256, 1, 11, 11]      885,248\n",
       "│    │    └─Sequential: 3-15             [32, 256, 1, 11, 11]      1,769,984\n",
       "│    │    └─Sequential: 3-16             [32, 256, 1, 11, 11]      33,280\n",
       "│    │    └─ReLU: 3-17                   [32, 256, 1, 11, 11]      --\n",
       "│    └─BasicBlock: 2-9                   [32, 256, 1, 11, 11]      --\n",
       "│    │    └─Sequential: 3-18             [32, 256, 1, 11, 11]      1,769,984\n",
       "│    │    └─Sequential: 3-19             [32, 256, 1, 11, 11]      1,769,984\n",
       "│    │    └─ReLU: 3-20                   [32, 256, 1, 11, 11]      --\n",
       "├─Sequential: 1-5                        [32, 512, 1, 6, 6]        --\n",
       "│    └─BasicBlock: 2-10                  [32, 512, 1, 6, 6]        --\n",
       "│    │    └─Sequential: 3-21             [32, 512, 1, 6, 6]        3,539,968\n",
       "│    │    └─Sequential: 3-22             [32, 512, 1, 6, 6]        7,078,912\n",
       "│    │    └─Sequential: 3-23             [32, 512, 1, 6, 6]        132,096\n",
       "│    │    └─ReLU: 3-24                   [32, 512, 1, 6, 6]        --\n",
       "│    └─BasicBlock: 2-11                  [32, 512, 1, 6, 6]        --\n",
       "│    │    └─Sequential: 3-25             [32, 512, 1, 6, 6]        7,078,912\n",
       "│    │    └─Sequential: 3-26             [32, 512, 1, 6, 6]        7,078,912\n",
       "│    │    └─ReLU: 3-27                   [32, 512, 1, 6, 6]        --\n",
       "├─AdaptiveAvgPool3d: 1-6                 [32, 512, 1, 1, 1]        --\n",
       "├─Linear: 1-7                            [32, 5]                   2,565\n",
       "==========================================================================================\n",
       "Total params: 33,168,837\n",
       "Trainable params: 33,168,837\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 202.98\n",
       "==========================================================================================\n",
       "Input size (MB): 10.84\n",
       "Forward/backward pass size (MB): 1571.55\n",
       "Params size (MB): 132.68\n",
       "Estimated Total Size (MB): 1715.07\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = mc3_18(weights = MC3_18_Weights.DEFAULT)\n",
    "model = r3d_18(weights = R3D_18_Weights.DEFAULT)\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "model.fc = nn.Linear(in_features=512, out_features=5, bias=True)\n",
    "summary(model, (32,3,4,84,84))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), 10e-5, weight_decay=0.1)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 3905/3905 [39:05<00:00,  1.67it/s, Loss=0.0099, Accuracy=0.892] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Val Loss: 0.0247, Val Accuracy: 0.7776\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    val_loss = 0.0\n",
    "    val_correct_predictions = 0\n",
    "    val_total_samples = 0\n",
    "\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # tqdm bar for progress visualization\n",
    "    pbar = tqdm(dataloader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=True)\n",
    "    for inputs, labels in pbar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs.to(device))\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update statistics\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(torch.softmax(outputs,1), 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        # Update tqdm bar with current loss and accuracy\n",
    "        pbar.set_postfix({'Loss': total_loss / total_samples, 'Accuracy': correct_predictions / total_samples})\n",
    "        steps = steps + 1\n",
    "\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs.to(device))\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Update statistics\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(torch.softmax(outputs,1), 1)\n",
    "            val_correct_predictions += (predicted == labels).sum().item()\n",
    "            val_total_samples += labels.size(0)\n",
    "\n",
    "    # Calculate and print epoch-level accuracy and loss for validation\n",
    "    epoch_loss = val_loss / val_total_samples\n",
    "    epoch_accuracy = val_correct_predictions / val_total_samples\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Val Loss: {epoch_loss:.4f}, Val Accuracy: {epoch_accuracy:.4f}')\n",
    "    torch.save(model.state_dict(), \"model_r3d.pth\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_r3d.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mc3_18(weights = MC3_18_Weights.DEFAULT)\n",
    "model.fc = nn.Linear(in_features=512, out_features=5, bias=True)\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), 10e-5, weight_decay=0.1)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 3905/3905 [35:54<00:00,  1.81it/s, Loss=0.00966, Accuracy=0.895]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Val Loss: 0.0241, Val Accuracy: 0.7747\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    val_loss = 0.0\n",
    "    val_correct_predictions = 0\n",
    "    val_total_samples = 0\n",
    "\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # tqdm bar for progress visualization\n",
    "    pbar = tqdm(dataloader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=True)\n",
    "    for inputs, labels in pbar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs.to(device))\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update statistics\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(torch.softmax(outputs,1), 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        # Update tqdm bar with current loss and accuracy\n",
    "        pbar.set_postfix({'Loss': total_loss / total_samples, 'Accuracy': correct_predictions / total_samples})\n",
    "        steps = steps + 1\n",
    "\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs.to(device))\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Update statistics\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(torch.softmax(outputs,1), 1)\n",
    "            val_correct_predictions += (predicted == labels).sum().item()\n",
    "            val_total_samples += labels.size(0)\n",
    "\n",
    "    # Calculate and print epoch-level accuracy and loss for validation\n",
    "    epoch_loss = val_loss / val_total_samples\n",
    "    epoch_accuracy = val_correct_predictions / val_total_samples\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Val Loss: {epoch_loss:.4f}, Val Accuracy: {epoch_accuracy:.4f}')\n",
    "    torch.save(model.state_dict(), \"model_mc3.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autodrive-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
